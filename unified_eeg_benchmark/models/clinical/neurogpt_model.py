from ..abstract_model import AbstractModel
from typing import List, Dict, cast, Literal
import numpy as np
import torch
import os
import random
import numpy as np
import pandas as pd
from mne.io import BaseRaw
from torch import manual_seed
from .NeuroGPT.src.batcher.downstream_dataset import MotorImageryDataset, BCIDataset
from .NeuroGPT.src.encoder.conformer_braindecode import EEGConformer
from .NeuroGPT.src.decoder.make_decoder import make_decoder
from .NeuroGPT.src.embedder.make import make_embedder
from .NeuroGPT.src.decoder.unembedder import make_unembedder
from .NeuroGPT.src.trainer.make import make_trainer
from .NeuroGPT.src.utils import cv_split_bci
from .NeuroGPT.src.model import Model
from sklearn.model_selection import train_test_split
from .LaBraM.make_dataset_2 import make_dataset as make_dataset_2
from .LaBraM.utils_2 import calc_class_weights, map_label_reverse
from joblib import Memory
from ...utils.config import get_config_value


def make_model(model_config: Dict=None, class_weights: List[float]=None) -> Model:
    """Make model from model_config 
    (as generated by get_config()).
    """
    chann_coords = None
    
    encoder = EEGConformer(n_outputs=model_config["num_decoding_classes"], n_chans=22, n_times=model_config['chunk_len'], ch_pos=chann_coords, is_decoding_mode=model_config["ft_only_encoder"])
    #calculates the output dimension of the encoder, which is the output of transformer layer.
    model_config["parcellation_dim"] = ((model_config['chunk_len'] - model_config['filter_time_length'] + 1 - model_config['pool_time_length']) // model_config['stride_avg_pool'] + 1) * model_config['n_filters_time']

    embedder = make_embedder(
        training_style=model_config["training_style"],
        architecture=model_config["architecture"],
        in_dim=model_config["parcellation_dim"], # flattened, channel x chunk length
        embed_dim=model_config["embedding_dim"],
        num_hidden_layers=model_config["num_hidden_layers_embedding_model"],
        dropout=model_config["dropout"],
        n_positions=model_config["n_positions"]
    )
    decoder = make_decoder(
        architecture=model_config["architecture"],
        num_hidden_layers=model_config["num_hidden_layers"],
        embed_dim=model_config["embedding_dim"],
        num_attention_heads=model_config["num_attention_heads"],
        n_positions=model_config["n_positions"],
        intermediate_dim_factor=model_config["intermediate_dim_factor"],
        hidden_activation=model_config["hidden_activation"],
        dropout=model_config["dropout"]
    )

    if model_config["embedding_dim"] != model_config["parcellation_dim"]:
        unembedder = make_unembedder(
            embed_dim=model_config["embedding_dim"],
            num_hidden_layers=model_config["num_hidden_layers_unembedding_model"],
            out_dim=model_config["parcellation_dim"],
            dropout=model_config["dropout"],
        )
    else:
        print("No Embedder and Unembedder!")
        unembedder = None

    model = Model(
        encoder=encoder,
        embedder=embedder,
        decoder=decoder,
        unembedder=unembedder
    )

    model.switch_ft_mode(ft_encoder_only=True)

    model.switch_decoding_mode(
        is_decoding_mode=True,
        num_decoding_classes=model_config["num_decoding_classes"]
    )

    model.from_pretrained(model_config["pretrained_model"])

    if model_config["freeze_embedder"]:
        for param in model.embedder.parameters():
            param.requires_grad = False

    if model_config["freeze_decoder"]:
        for param in model.decoder.parameters():
            param.requires_grad = False

    if model_config["freeze_encoder"]:
        for name, param in model.encoder.named_parameters():
            if 'fc.' in name \
            or 'final_layer' in name:
                continue
            else:
                param.requires_grad = False

    if 'freeze_decoder_without_pooler_heads' in model_config \
        and model_config["freeze_decoder_without_pooler_heads"]:
        for name, param in model.decoder.named_parameters():
            if 'pooler_layer' in name \
            or 'decoding_head' in name \
            or 'is_next_head' in name:
                continue
            else:
                param.requires_grad = False

    if model_config["freeze_unembedder"] and unembedder is not None:
        for param in model.unembedder.parameters():
            param.requires_grad = False

    if class_weights is not None:
        model.embedder.xe_loss = torch.nn.CrossEntropyLoss(reduction='mean', weight=class_weights)
    
    return model


class NeuroGPTModel(AbstractModel):
    def __init__(
        self,
        seed: int = 42,
    ):
        super().__init__("NeuroGPTModel")
        assert torch.cuda.is_available(), "CUDA is not available"
        self.use_cache = True
        self.chunk_len_s = None

        self.config = {
            "training_style": "decoding",
            "num_decoding_classes": 2,
            "training_steps": 3000,
            "eval_every_n_steps": 1000,
            "log_every_n_steps": 500,
            "num_chunks": 2,
            "per_device_training_batch_size": 32,
            "per_device_validation_batch_size": 32,
            "chunk_len": 500,
            "chunk_ovlp": 0,
            "run_name": "dst",
            "ft_only_encoder": True,
            "fold_i": 0,
            "num_encoder_layers": 6,
            "num_hidden_layers": 6,
            "learning_rate": 1e-5,
            "use_encoder": True,
            "embedding_dim": 1024,
            "pretrained_model": '/itet-stor/jbuerki/home/unified_eeg_benchmark/unified_eeg_benchmark/models/clinical/NeuroGPT/pretrained_model/pytorch_model.bin',
            "dst_data_path": "./models/NeuroGPT/bci2a_eeg_npz/",
            # Additional parameters with reasonable defaults:
            "do_train": True,
            "resume_from": None,
            "architecture": "GPT",
            "num_hidden_layers_embedding_model": 1,
            "freeze_embedder": False,
            "num_hidden_layers_unembedding_model": 1,
            "freeze_unembedder": False,
            "num_attention_heads": 16,  # 1024 // 64
            "intermediate_dim_factor": 4,
            "hidden_activation": "gelu_new",
            "freeze_decoder": False,
            "freeze_decoder_without_pooler_heads": False,
            "freeze_encoder": False,
            "log_dir": "results/models/upstream",
            "wandb_mode": "disabled",
            "wandb_project_name": "learning-from-brains",
            "seed": seed,
            "set_seed": True,
            "fp16": True,
            "deepspeed": None,
            "local_rank": -1,
            "num_workers": 2,
            "plot_model_graph": False,
            "smoke_test": False,
            "bold_dummy_mode": False,
            "do_normalization": True,
            "filter_time_length": 25,
            "pool_time_length": 75,
            "stride_avg_pool": 15,
            "n_filters_time": 40,
            "dropout": 0.1,
            "n_positions": 512,
            "lr_scheduler": "linear",
            "max_grad_norm": 1.0,
            "adam_beta_1": 0.9,
            "adam_beta_2": 0.999,
            "adam_epsilon": 1e-8,
            "weight_decay": 0.1,
            "optim": "adamw_hf",
            "warmup_ratio": 0.1,
        }

        os.makedirs(
            self.config["log_dir"],
            exist_ok=True
        )
        
        if self.config['set_seed']:
            random.seed(self.config["seed"])
            manual_seed(self.config["seed"])

    def fit(self, X: List[np.ndarray|List[BaseRaw]], y: List[np.ndarray|List[str]], meta: List[Dict]) -> None:
        print("inside fit")
        
        task_name = meta[0]["task_name"]
        kwargs = {
            "sample_keys": [
                'inputs',
                'attention_mask'
            ],
            "chunk_len": self.config["chunk_len"],
            "num_chunks": self.config["num_chunks"],
            "ovlp": self.config["chunk_ovlp"],
            "root_path": "",
            "gpt_only": not self.config["use_encoder"]
        }
        train_dataset  = make_dataset_2(X, y, meta, task_name, self.name, self.chunk_len_s, is_train=True, use_cache=self.use_cache, **kwargs)

        val_split = 0.2
        if val_split is not None:
            train_dataset, val_dataset = train_dataset.split_train_val(val_split)
        else:
            val_dataset = None

        class_weights = torch.tensor(calc_class_weights(y))

        def model_init(params: Dict=None):
            model_config = dict(self.config)
            if params is not None:
                model_config |= params

            return make_model(model_config, class_weights)

        model_save_steps = self.config["training_steps"]*2
            

        self.trainer = make_trainer(
            model_init=model_init,
            training_style=self.config["training_style"],
            run_name=self.config["run_name"],
            output_dir=self.config["log_dir"],
            train_dataset=train_dataset,
            validation_dataset=val_dataset,
            per_device_train_batch_size=self.config["per_device_training_batch_size"],
            per_device_eval_batch_size=self.config["per_device_validation_batch_size"],
            dataloader_num_workers=self.config["num_workers"],
            optim=self.config["optim"],
            learning_rate=self.config["learning_rate"],
            weight_decay=self.config["weight_decay"],
            adam_beta1=self.config["adam_beta_1"],
            adam_beta2=self.config["adam_beta_1"],
            adam_epsilon=self.config["adam_epsilon"],
            max_grad_norm=self.config["max_grad_norm"],
            lr_scheduler_type=self.config["lr_scheduler"],
            warmup_ratio=self.config["warmup_ratio"],
            max_steps=self.config["training_steps"],
            # num_train_epochs=5,
            save_steps=model_save_steps,
            logging_steps=self.config["log_every_n_steps"],
            eval_steps=self.config["eval_every_n_steps"],
            seed=self.config["seed"] if self.config['set_seed'] else np.random.choice(range(1, 100000)),
            fp16=self.config["fp16"],
            deepspeed=self.config["deepspeed"],
        )

        self.trainer.train(resume_from_checkpoint=self.config["resume_from"])
        self.trainer.save_model(
            os.path.join(
                self.config["log_dir"],
                'model_final'
            )
        )

    def predict(self, X: List[np.ndarray|List[BaseRaw]], meta: List[Dict]) -> np.ndarray:
        print("inside predict")

        task_name = meta[0]["task_name"]

        kwargs = {
            "sample_keys": [
                'inputs',
                'attention_mask'
            ],
            "chunk_len": self.config["chunk_len"],
            "num_chunks": self.config["num_chunks"],
            "ovlp": self.config["chunk_ovlp"],
            "root_path": "",
            "gpt_only": not self.config["use_encoder"]
        }
        self.test_dataset  = make_dataset_2(X, None, meta, task_name, self.name, self.chunk_len_s, is_train=False, use_cache=self.use_cache, **kwargs)

        test_prediction = self.trainer.predict(self.test_dataset)

        if test_prediction.label_ids is None or self.chunk_len_s is None:
            def softmax(logits):
                exp_logits = np.exp(logits - np.max(logits, axis=-1, keepdims=True))  # for numerical stability
                return exp_logits / np.sum(exp_logits, axis=-1, keepdims=True)

            # Convert logits to probabilities
            probabilities = softmax(test_prediction.predictions["outputs"])

            # Get the predicted classes (0 or 1) by taking argmax along axis 1 (the class axis)
            pred = np.argmax(probabilities, axis=1)
        else:
            # TODO
            pred = test_prediction.label_ids

        pred = np.array([map_label_reverse(p, task_name) for p in pred])
 
        print(pred)
        return pred
        
